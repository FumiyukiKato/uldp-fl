{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personalized User-level DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autodp import rdp_acct\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "import os\n",
    "path_project = os.path.dirname(os.path.abspath('.'))\n",
    "import sys\n",
    "sys.path.append(os.path.join(path_project, 'src'))\n",
    "sys.path.append(os.path.join(path_project, 'exp/script'))\n",
    "\n",
    "import options\n",
    "\n",
    "img_path = os.path.join(path_project, 'exp', 'img')\n",
    "pickle_path = os.path.join(path_project, 'exp', 'pickle')\n",
    "results_path = os.path.join(path_project, 'exp', 'results')\n",
    "default_args = options.build_default_args(path_project)\n",
    "\n",
    "import copy\n",
    "from run_simulation import run_simulation\n",
    "\n",
    "from mylogger import logger_set_debug, logger_set_info, logger_set_warning\n",
    "from opacus.accountants import RDPAccountant\n",
    "\n",
    "import pickle\n",
    "\n",
    "def RDP_gaussian_with_C(sigma, alpha, C):\n",
    "    assert(sigma > 0)\n",
    "    assert(alpha >= 0)\n",
    "    return 0.5 * C**2 / sigma ** 2 * alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_LIST_SIZE = 30\n",
    "\n",
    "# binary search given q_u\n",
    "def from_q_u(q_u, delta, epsilon_u, sigma, T, m=100, precision=1e-6):\n",
    "    max_sensitivity_u = 100\n",
    "    min_sensitivity_u = 0\n",
    "    while True:\n",
    "        sensitivity_u = (max_sensitivity_u + min_sensitivity_u) / 2\n",
    "        # func_gaussian = lambda x: RDP_gaussian_with_C(sigma, x, sensitivity_u)\n",
    "        # accountant = rdp_acct.anaRDPacct(m=m)\n",
    "        accountant = RDPAccountant()\n",
    "        for i in range(T):\n",
    "            accountant.step(noise_multiplier=sigma/sensitivity_u, sample_rate=q_u)\n",
    "            # accountant.compose_subsampled_mechanisms_lowerbound(func=func_gaussian, prob=q_u)\n",
    "        # eps = accountant.get_eps(delta)\n",
    "        eps = accountant.get_epsilon(delta=delta)\n",
    "        if eps < epsilon_u:\n",
    "            min_sensitivity_u = sensitivity_u\n",
    "        else:\n",
    "            max_sensitivity_u = sensitivity_u\n",
    "        if 0 < epsilon_u - eps and epsilon_u - eps < precision:\n",
    "            return sensitivity_u, eps\n",
    "\n",
    "def from_q_u_with_accountant(q_u, delta, epsilon_u, sigma, current_accountant: RDPAccountant, T, precision=1e-6):\n",
    "    max_sensitivity_u = 100\n",
    "    min_sensitivity_u = 0\n",
    "    while True:\n",
    "        sensitivity_u = (max_sensitivity_u + min_sensitivity_u) / 2\n",
    "        accountant = copy.deepcopy(current_accountant)\n",
    "        for i in range(T):\n",
    "            accountant.step(noise_multiplier=sigma/sensitivity_u, sample_rate=q_u)\n",
    "        eps = accountant.get_epsilon(delta=delta)\n",
    "        if eps < epsilon_u:\n",
    "            min_sensitivity_u = sensitivity_u\n",
    "        else:\n",
    "            max_sensitivity_u = sensitivity_u\n",
    "        if 0 < epsilon_u - eps and epsilon_u - eps < precision:\n",
    "            return sensitivity_u, eps\n",
    "\n",
    "# binary search given_sensitivity_u\n",
    "def from_sensitivity_u(sensitivity_u, delta, epsilon_u, sigma, T, m=100, precision=1e-6):\n",
    "    max_q_u = 1.0\n",
    "    min_q_u = 0\n",
    "    # func_gaussian = lambda x: RDP_gaussian_with_C(sigma, x, sensitivity_u)\n",
    "    while True:\n",
    "        q_u = (max_q_u + min_q_u) / 2\n",
    "        # accountant = rdp_acct.anaRDPacct(m=m)\n",
    "        accountant = RDPAccountant()\n",
    "        for i in range(T):\n",
    "            # accountant.compose_subsampled_mechanisms_lowerbound(func=func_gaussian, prob=q_u)\n",
    "            accountant.step(noise_multiplier=sigma/sensitivity_u, sample_rate=q_u)\n",
    "        # eps = accountant.get_eps(delta)\n",
    "        eps = accountant.get_epsilon(delta=delta)\n",
    "        if eps < epsilon_u:\n",
    "            min_q_u = q_u\n",
    "        else:\n",
    "            max_q_u = q_u\n",
    "        if 0 < epsilon_u - eps and epsilon_u - eps < precision:\n",
    "            return q_u, eps\n",
    "        \n",
    "\n",
    "# qCカーブを書くために，適当にqを選んでCを計算して点をプロットする\n",
    "def make_q_c_curve(epsilon_u, delta, sigma, n_round=100, num_points=20, min=-5):\n",
    "    T = n_round\n",
    "\n",
    "    num_points = num_points // 3 * 2\n",
    "    x = np.logspace(min, -1, num_points).tolist() + np.linspace(0.15, 1.0, int(num_points/2)).tolist()\n",
    "    y = []\n",
    "    for q_u in x:\n",
    "        sensitivity_u, eps = from_q_u(q_u=q_u, delta=delta, epsilon_u=epsilon_u, sigma=sigma, T=T)\n",
    "        assert eps <= epsilon_u, f\"eps={eps} > epsilon_u={epsilon_u}\"\n",
    "        # print(\"sensitivity_u =\", sensitivity_u, \"eps =\", eps)\n",
    "        y.append(sensitivity_u)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def plot_q_c_curve(x, y, title=\"\", img_name=\"\", log=True):\n",
    "    fig = plt.figure(figsize=(8, 5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(x, y, marker='o', label='sensitivity_u')\n",
    "    # for i, (xi, yi) in enumerate(zip(x, y)):\n",
    "    #     if i % 5 == 0:\n",
    "    #         ax.annotate(f\"({xi:.5f}, {yi:.5f})\", (xi, yi), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    # # q*C の値をプロット\n",
    "    # ax.plot(x, np.array(x)*np.array(y), marker='x', linestyle='--', color='red', label='product_x*y')\n",
    "    # print(\"Max idx\", np.argmax(np.array(x)*np.array(y)))\n",
    "    # if log:\n",
    "    #     ax.set_xscale(\"log\")\n",
    "    #     ax.set_yscale(\"log\")\n",
    "    ax.set_xlabel(r'$q_u$', fontsize=20)\n",
    "    ax.set_ylabel(r'$C_u$', fontsize=20)\n",
    "    ax.tick_params(axis='both', labelsize=20)\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    fig.savefig(\n",
    "        os.path.join(img_path, \"q_c_pair\" + img_name + \".png\"),\n",
    "        dpi=150,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def make_static_params(epsilon_u_dct, delta, sigma, n_round, idx_per_group, q_step_size):\n",
    "    \"\"\"\n",
    "    Need to specify idx and q_step_size\n",
    "    \"\"\"\n",
    "    C_u_dct = {}\n",
    "    q_u_dct = {}\n",
    "\n",
    "    C_and_q_per_group = {}\n",
    "    for group_eps, idx in idx_per_group.items():\n",
    "        n_of_q_u = Q_LIST_SIZE\n",
    "        q_u_list = []\n",
    "        init_q_u = 1.0\n",
    "        for _ in range(n_of_q_u):\n",
    "            q_u_list.append(init_q_u)\n",
    "            init_q_u *= q_step_size\n",
    "        q_u = q_u_list[idx]\n",
    "        C_u, _eps = from_q_u(q_u=q_u, delta=delta, epsilon_u=group_eps, sigma=sigma, T=n_round)\n",
    "        assert _eps <= group_eps, f\"_eps={_eps} > eps_u={group_eps}\"\n",
    "        C_and_q_per_group[group_eps] = (C_u, q_u)\n",
    "\n",
    "    for user_id, eps_u in epsilon_u_dct.items():\n",
    "        C_u, q_u = C_and_q_per_group[eps_u]\n",
    "        C_u_dct[user_id] = C_u\n",
    "        q_u_dct[user_id] = q_u\n",
    "\n",
    "    return C_u_dct, q_u_dct\n",
    "\n",
    "\n",
    "def fed_simulation(\n",
    "    delta, \n",
    "    sigma, \n",
    "    n_users, \n",
    "    C_u=None, \n",
    "    q_u=None, \n",
    "    q_step_size=None,\n",
    "    momentum_weight=None,\n",
    "    times=1, \n",
    "    user_dist=\"uniform-iid\", \n",
    "    silo_dist=\"uniform\", \n",
    "    dataset_name=\"light_mnist\", \n",
    "    global_learning_rate=10.0, \n",
    "    clipping_bound=1.0,\n",
    "    n_round=10, \n",
    "    local_epochs=50, \n",
    "    local_learning_rate=0.01,\n",
    "    agg_strategy=\"PULDP-AVG\",\n",
    "    epsilon_u=None,\n",
    "    group_thresholds=None,\n",
    "    validation_ratio=0.0,\n",
    "    with_momentum=True,\n",
    "    off_train_loss_noise=False,\n",
    "    step_decay=True,\n",
    "    hp_baseline=None,\n",
    "):\n",
    "    args = options.build_default_args(path_project)\n",
    "\n",
    "    if dataset_name == \"heart_disease\":\n",
    "        from flamby_utils.heart_disease import update_args\n",
    "\n",
    "        args = update_args(args)\n",
    "\n",
    "    elif dataset_name == \"tcga_brca\":\n",
    "        from flamby_utils.tcga_brca import update_args\n",
    "\n",
    "        args = update_args(args)\n",
    "\n",
    "    args.dataset_name = dataset_name\n",
    "    args.agg_strategy = agg_strategy\n",
    "    args.n_total_round = n_round\n",
    "    args.n_users = n_users\n",
    "    args.local_epochs = local_epochs\n",
    "    args.times = times\n",
    "\n",
    "    args.user_dist = user_dist\n",
    "    args.silo_dist = silo_dist\n",
    "    args.global_learning_rate = global_learning_rate\n",
    "    args.local_learning_rate = local_learning_rate\n",
    "    args.clipping_bound = clipping_bound\n",
    "    args.with_momentum = with_momentum\n",
    "    args.momentum_weight = momentum_weight\n",
    "    args.off_train_loss_noise = off_train_loss_noise\n",
    "    args.step_decay = step_decay\n",
    "    args.hp_baseline = hp_baseline\n",
    "\n",
    "    args.delta = delta\n",
    "    args.sigma = sigma\n",
    "    args.C_u = C_u\n",
    "    args.q_u = q_u\n",
    "    args.q_step_size = q_step_size\n",
    "    args.epsilon_u = epsilon_u\n",
    "    args.group_thresholds = group_thresholds\n",
    "    args.dry_run = False\n",
    "    args.secure_w = False\n",
    "\n",
    "    args.validation_ratio = validation_ratio\n",
    "    # args.client_optimizer = \"sgd\"\n",
    "    args.client_optimizer = \"adam\"\n",
    "\n",
    "    results_list = []\n",
    "    for i in range(args.times):\n",
    "        print(\"======== TIME:\", i, \"start\")\n",
    "        args.seed = args.seed + i\n",
    "        # try:\n",
    "        sim_results = run_simulation(args, path_project)\n",
    "        results_list.append(sim_results)\n",
    "        # except AssertionError:\n",
    "        #     results_list.append(\"Assertion Error\")\n",
    "\n",
    "    return results_list\n",
    "\n",
    "\n",
    "def calc_metric(results, symbol=\"test\"):\n",
    "    if symbol == \"train\":\n",
    "        acc_list = np.array([r['train'][f'train_metric'] for r in results])\n",
    "        loss_list = np.array([r['train'][f'train_loss'] for r in results])\n",
    "    else:\n",
    "        acc_list = np.array([r['global'][f'global_{symbol}'][-1][1] for r in results])\n",
    "        loss_list = np.array([r['global'][f'global_{symbol}'][-1][2] for r in results])\n",
    "    acc_mean, acc_std, loss_mean, loss_std = np.mean(acc_list), np.std(acc_list), np.mean(loss_list), np.std(loss_list)\n",
    "    return acc_mean, acc_std, loss_mean, loss_std\n",
    "\n",
    "\n",
    "def make_epsilon_u(epsilon=1.0, n_users=0, dist=\"homo\", epsilon_list=[], ratio_list=[], random_state: np.random.RandomState=None) -> dict[int, float]:\n",
    "    if dist == \"homo\":\n",
    "        epsilon_u = {user_id: epsilon for user_id in range(n_users)}\n",
    "    elif dist == \"hetero\":\n",
    "        assert len(epsilon_list) > 0 and len(ratio_list) > 0\n",
    "        epsilon_u_list = random_state.choice(epsilon_list, size=n_users, p=ratio_list)\n",
    "        epsilon_u = {user_id: epsilon_u_list[user_id] for user_id in range(n_users)}\n",
    "    else:\n",
    "        raise ValueError(f\"invalid dist {dist}\")\n",
    "    return epsilon_u\n",
    "\n",
    "def group_by_closest_below(epsilon_u_dct: dict, group_thresholds: list):\n",
    "    minimum = min(epsilon_u_dct.values())\n",
    "    group_thresholds = set(group_thresholds) | {minimum}\n",
    "    grouped = {\n",
    "        g: [] for g in group_thresholds\n",
    "    }  # Initialize the dictionary with empty lists for each group threshold\n",
    "    for key, value in epsilon_u_dct.items():\n",
    "        # Find the closest group threshold that is less than or equal to the value\n",
    "        closest_group = max([g for g in group_thresholds if g <= value], default=None)\n",
    "        # If a suitable group is found, append the key to the corresponding list\n",
    "        if closest_group is not None:\n",
    "            grouped[closest_group].append(key)\n",
    "\n",
    "    return grouped\n",
    "\n",
    "\n",
    "## STATIC MANUAL OPTIMIZATION\n",
    "\n",
    "def prepare_grid_search(epsilon_u, start_idx: int, end_idx: int):\n",
    "    # set idx list for each group\n",
    "    group_eps_set = set(epsilon_u.values())\n",
    "    idx_per_group_list = []\n",
    "\n",
    "    idx_list = list(range(Q_LIST_SIZE))[start_idx:end_idx]\n",
    "    idx_list_list = [idx_list for _ in range(len(group_eps_set))]\n",
    "\n",
    "    for combination in itertools.product(*idx_list_list):\n",
    "        idx_per_group = {}\n",
    "        for idx, group_eps in zip(combination, group_eps_set):\n",
    "            idx_per_group[group_eps] = idx\n",
    "        idx_per_group_list.append(idx_per_group)\n",
    "\n",
    "    return {\"name\": \"grid\", \"params\": {\"idx_per_group_list\": idx_per_group_list}}\n",
    "\n",
    "\n",
    "def prepare_random_search(epsilon_u, start_idx: int, end_idx: int, random_state: np.random.RandomState, n_samples: int):\n",
    "    # set idx list for each group``\n",
    "    group_eps_set = set(epsilon_u.values())\n",
    "    idx_per_group_list = []\n",
    "\n",
    "    idx_list = list(range(Q_LIST_SIZE))[start_idx:end_idx]\n",
    "    idx_list_list = [idx_list for _ in range(len(group_eps_set))]\n",
    "    all_candidates = itertools.product(*idx_list_list)\n",
    "    samples = random_state.choice(len(all_candidates), size=n_samples, replace=False)\n",
    "\n",
    "    for sample in samples:\n",
    "        idx_per_group = {}\n",
    "        for idx, group_eps in zip(all_candidates[sample], group_eps_set):\n",
    "            idx_per_group[group_eps] = idx\n",
    "        idx_per_group_list.append(idx_per_group)\n",
    "\n",
    "    return {\"name\": \"random\", \"params\": {\"idx_per_group_list\": idx_per_group_list}}\n",
    "\n",
    "\n",
    "def prepare_independent_search(epsilon_u, start_idx: int, end_idx: int):\n",
    "    # set idx list for each group\n",
    "    group_eps_set = set(epsilon_u.values())\n",
    "    idx_per_group_list = []\n",
    "    idx_list = list(range(Q_LIST_SIZE))[start_idx:end_idx]\n",
    "\n",
    "    for group_eps in group_eps_set:\n",
    "        for idx in idx_list:\n",
    "            idx_per_group = {}\n",
    "            idx_per_group[group_eps] = idx\n",
    "            for group_eps in group_eps_set:\n",
    "                if group_eps != group_eps:\n",
    "                    idx_per_group[group_eps] = int((start_idx + end_idx) / 2)\n",
    "        idx_per_group_list.append(idx_per_group)\n",
    "\n",
    "    return {\"name\": \"independent\", \"params\": {\"idx_per_group_list\": idx_per_group_list}}\n",
    "\n",
    "\n",
    "def static_optimization(epsilon_u, sigma, delta, n_users, n_round, dataset_name, times, q_step_size, opt_strategy: dict, global_learning_rate=10.0, local_learning_rate=0.01, local_epochs=30, validation_ratio=0.0):\n",
    "    try:\n",
    "        prefix_epsilon_u = list(epsilon_u.items())[:4]\n",
    "        with open(os.path.join(pickle_path, f'static_optimization_{sigma}_{delta}_{n_users}_{n_round}_{dataset_name}_{q_step_size}_{opt_strategy[\"name\"]}_{validation_ratio}_{prefix_epsilon_u}.pkl'), 'rb') as file:\n",
    "            results_dict = pickle.load(file)\n",
    "    except FileNotFoundError:\n",
    "        results_dict = {}\n",
    "\n",
    "    if opt_strategy[\"name\"] in [\"grid\", \"random\", \"independent\"]:\n",
    "        # grid search\n",
    "        for idx_per_group in opt_strategy[\"params\"][\"idx_per_group_list\"]:\n",
    "            print(\"IDX: \", idx_per_group)\n",
    "            C_u, q_u = make_static_params(epsilon_u, delta, sigma, n_round, idx_per_group=idx_per_group, q_step_size=q_step_size)\n",
    "            result = fed_simulation(\n",
    "                delta, sigma, n_users, C_u=C_u, q_u=q_u, agg_strategy=\"PULDP-AVG\",\n",
    "                times=times, n_round=n_round, user_dist=\"zipf-iid\", silo_dist=\"zipf\", \n",
    "                global_learning_rate=global_learning_rate, local_learning_rate=local_learning_rate, dataset_name=dataset_name,\n",
    "                local_epochs=local_epochs, epsilon_u=epsilon_u, validation_ratio=validation_ratio,\n",
    "            )\n",
    "            results_dict[str(idx_per_group)] = (q_u, C_u, result)\n",
    "    else:\n",
    "        raise ValueError(f\"invalid opt_strategy {opt_strategy}\")\n",
    "    \n",
    "    with open(os.path.join(pickle_path, f'static_optimization_{sigma}_{delta}_{n_users}_{n_round}_{dataset_name}_{q_step_size}_{opt_strategy[\"name\"]}_{validation_ratio}_{prefix_epsilon_u}.pkl'), 'wb') as file:\n",
    "        pickle.dump(results_dict, file)\n",
    "\n",
    "\n",
    "def show_static_optimization_result(epsilon_u, sigma, delta, n_users, n_round, dataset_name, q_step_size, opt_strategy: dict, n_silos, validation_ratio=0.0, train_loss=False, errorbar=True, img_name=\"\"):\n",
    "    prefix_epsilon_u = list(epsilon_u.items())[:4]\n",
    "    with open(os.path.join(pickle_path, f'static_optimization_{sigma}_{delta}_{n_users}_{n_round}_{dataset_name}_{q_step_size}_{opt_strategy[\"name\"]}_{validation_ratio}_{prefix_epsilon_u}.pkl'), 'rb') as file:\n",
    "        results_dict = pickle.load(file)\n",
    "\n",
    "    plt.figure(figsize=(14, 5)) \n",
    "    x = list(results_dict.keys())\n",
    "    q_u_list = [results_dict[i][0][0] for i in x]\n",
    "    C_u_list = [results_dict[i][1][0] for i in x]\n",
    "    acc_mean_acc_std_loss_mean_loss_std = [calc_metric(results_dict[i][2], \"test\") for i in x]\n",
    "    y = [acc_mean_acc_std_loss_mean_loss_std[i][2] for i in range(len(x))]  # loss_mean\n",
    "    error = [acc_mean_acc_std_loss_mean_loss_std[i][3] for i in range(len(x))]  # loss_std\n",
    "    plt.title(r'{}: $\\epsilon_u={}$, $\\sigma={}$, $|U|={}$, $|S|={}$, $T={}$'.format(dataset_name, prefix_epsilon_u[0][1], sigma, n_users, n_silos, n_round), fontsize=20)\n",
    "    plt.ylabel('Test Loss', fontsize=20)\n",
    "    if errorbar:\n",
    "        plt.errorbar(x, y, yerr=error, fmt='-o', label='Test Loss')\n",
    "    else:\n",
    "        plt.plot(x, y, '-o', label='Test Loss')\n",
    "    for i in range(len(x)):\n",
    "        plt.text(x[i], y[i]*1.02, f'q={q_u_list[i]:.3f}\\nC={C_u_list[i]:.3f}', fontsize=10)\n",
    "    plt.legend(loc='upper left', fontsize=16)\n",
    "    plt.yscale('log')\n",
    "\n",
    "    ax2 = plt.twinx()\n",
    "    y_metric = [acc_mean_acc_std_loss_mean_loss_std[i][0] for i in range(len(x))]  # acc_mean\n",
    "    error_metric = [acc_mean_acc_std_loss_mean_loss_std[i][1] for i in range(len(x))]  # acc_std\n",
    "    if errorbar:\n",
    "        ax2.errorbar(x, y_metric, yerr=error_metric, fmt='-o', color='red', label='Accuracy')\n",
    "    else:\n",
    "        ax2.plot(x, y_metric, '-o', color='red', label='Accuracy')\n",
    "    ax2.set_ylabel(\"Accuracy\", fontsize=20)\n",
    "    ax2.legend(loc='upper right', fontsize=16)\n",
    "    plt.xticks([])\n",
    "    \n",
    "    plt.savefig(\n",
    "        os.path.join(img_path, \"static_optimization_result-\" + img_name + \".png\"),\n",
    "        dpi=150,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    # optimize for test loss, not for validation loss!\n",
    "    min_idx, min_test_loss = x[np.argmin(y)], np.min(y)\n",
    "\n",
    "    if train_loss:\n",
    "        plt.figure(figsize=(8, 3))\n",
    "        x = list(results_dict.keys())\n",
    "        q_u_list = [results_dict[i][0][0] for i in x]\n",
    "        C_u_list = [results_dict[i][1][0] for i in x]\n",
    "        acc_mean_acc_std_loss_mean_loss_std = [calc_metric(results_dict[i][2], \"train\") for i in x]\n",
    "        y = [acc_mean_acc_std_loss_mean_loss_std[i][2] for i in range(len(x))]  # loss_mean\n",
    "        error = [acc_mean_acc_std_loss_mean_loss_std[i][3] for i in range(len(x))]  # loss_std\n",
    "        if errorbar:\n",
    "            plt.errorbar(x, y, yerr=error, fmt='-o')\n",
    "        else:\n",
    "            plt.plot(x, y, '-o')\n",
    "        for i in range(len(x)):\n",
    "            plt.text(x[i], y[i]*1.02, f'q={q_u_list[i]:.3f}\\nC={C_u_list[i]:.3f}', fontsize=8)\n",
    "        plt.title('Train Loss Mean with Standard Deviation over different idx')\n",
    "        plt.xlabel('idx')\n",
    "        plt.ylabel('Train Loss Mean')\n",
    "        plt.legend(loc='upper right')\n",
    "\n",
    "        ax2 = plt.twinx()\n",
    "        y_metric = [acc_mean_acc_std_loss_mean_loss_std[i][0] for i in range(len(x))]  # acc_mean\n",
    "        error_metric = [acc_mean_acc_std_loss_mean_loss_std[i][1] for i in range(len(x))]  # acc_std\n",
    "        if errorbar:\n",
    "            ax2.errorbar(x, y_metric, yerr=error_metric, fmt='-o', color='red', label='Accuracy')\n",
    "        else:\n",
    "            ax2.plot(x, y_metric, '-o', color='red', label='Accuracy')\n",
    "\n",
    "        ax2.legend(loc='lower left')\n",
    "        plt.show()\n",
    "\n",
    "    if validation_ratio > 0.0:\n",
    "        plt.figure(figsize=(8, 3)) \n",
    "        x = list(results_dict.keys())\n",
    "        q_u_list = [results_dict[i][0][0] for i in x]\n",
    "        C_u_list = [results_dict[i][1][0] for i in x]\n",
    "        acc_mean_acc_std_loss_mean_loss_std = [calc_metric(results_dict[i][2], \"valid\") for i in x]\n",
    "        y = [acc_mean_acc_std_loss_mean_loss_std[i][2] for i in range(len(x))]  # loss_mean\n",
    "        error = [acc_mean_acc_std_loss_mean_loss_std[i][3] for i in range(len(x))]  # loss_std\n",
    "        if errorbar:\n",
    "            plt.errorbar(x, y, yerr=error, fmt='-o')\n",
    "        else:\n",
    "            plt.plot(x, y, '-o')\n",
    "        for i in range(len(x)):\n",
    "            plt.text(x[i], y[i]*1.02, f'q={q_u_list[i]:.3f}\\nC={C_u_list[i]:.3f}', fontsize=8)\n",
    "        plt.title('Valid Loss Mean with Standard Deviation over different idx')\n",
    "        plt.xlabel('idx')\n",
    "        plt.ylabel('Valid Loss Mean')\n",
    "        plt.legend(loc='upper right')\n",
    "\n",
    "        ax2 = plt.twinx()\n",
    "        y_metric = [acc_mean_acc_std_loss_mean_loss_std[i][0] for i in range(len(x))]  # acc_mean\n",
    "        error_metric = [acc_mean_acc_std_loss_mean_loss_std[i][1] for i in range(len(x))]  # acc_std\n",
    "        if errorbar:\n",
    "            ax2.errorbar(x, y_metric, yerr=error_metric, fmt='-o', color='red', label='Accuracy')\n",
    "        else:\n",
    "            ax2.plot(x, y_metric, '-o', color='red', label='Accuracy')\n",
    "        ax2.legend(loc='lower left')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    return min_idx, min_test_loss\n",
    "\n",
    "\n",
    "def run_with_specified_idx(epsilon_u, sigma, delta, n_users, n_round, dataset_name, q_step_size, times, idx_per_group, global_learning_rate=10.0, local_learning_rate=0.01, local_epochs=30, validation_ratio=0.0):\n",
    "    C_u, q_u = make_static_params(epsilon_u, delta, sigma, n_round, idx_per_group=idx_per_group, q_step_size=q_step_size)\n",
    "    result = fed_simulation(\n",
    "        delta, sigma, n_users, C_u=C_u, q_u=q_u, agg_strategy=\"PULDP-AVG\",\n",
    "        times=times, n_round=n_round, user_dist=\"zipf-iid\", silo_dist=\"zipf\", \n",
    "        global_learning_rate=global_learning_rate, local_learning_rate=local_learning_rate, dataset_name=dataset_name,\n",
    "        local_epochs=local_epochs, epsilon_u=epsilon_u, validation_ratio=validation_ratio,\n",
    "    )\n",
    "\n",
    "    acc_mean, acc_std, loss_mean, loss_std = calc_metric(result, \"test\")\n",
    "    print(f\"TEST ACC: {acc_mean:.4f} ± {acc_std:.4f}\", f\", TEST LOSS: {loss_mean:.4f} ± {loss_std:.4f}\")\n",
    "\n",
    "    if validation_ratio > 0.0:\n",
    "        acc_mean, acc_std, loss_mean, loss_std = calc_metric(result, \"valid\")\n",
    "        print(f\"VALID ACC: {acc_mean:.4f} ± {acc_std:.4f}\", f\", VALID LOSS: {loss_mean:.4f} ± {loss_std:.4f}\")\n",
    "\n",
    "    prefix_epsilon_u = list(epsilon_u.items())[:4]\n",
    "    with open(os.path.join(pickle_path, f'specified_idx_{n_users}_{sigma}_{delta}_{dataset_name}_{n_round}_{idx_per_group}_{q_step_size}_{validation_ratio}_{prefix_epsilon_u}.pkl'), 'wb') as file:\n",
    "        pickle.dump(result, file)\n",
    "\n",
    "## ONLINE OPTIMIZATION\n",
    "\n",
    "def run_online_optimization(\n",
    "    epsilon_u, sigma, delta, n_users, n_round, dataset_name, q_step_size, group_thresholds, times, agg_strategy,\n",
    "    global_learning_rate=10.0, local_learning_rate=0.01, local_epochs=30, validation_ratio=0.0, \n",
    "    with_momentum=None, step_decay=True, hp_baseline=None, off_train_loss_noise=None,\n",
    "):\n",
    "    result = fed_simulation(\n",
    "        delta, sigma, n_users, C_u=None, q_u=None, q_step_size=q_step_size, agg_strategy=agg_strategy,\n",
    "        times=times, n_round=n_round, user_dist=\"zipf-iid\", silo_dist=\"zipf\", \n",
    "        global_learning_rate=global_learning_rate, local_learning_rate=local_learning_rate, dataset_name=dataset_name,\n",
    "        local_epochs=local_epochs, epsilon_u=epsilon_u, group_thresholds=group_thresholds, validation_ratio=validation_ratio,\n",
    "        with_momentum=with_momentum, step_decay=step_decay, off_train_loss_noise=off_train_loss_noise, hp_baseline=hp_baseline,\n",
    "    )\n",
    "\n",
    "    acc_mean, acc_std, loss_mean, loss_std = calc_metric(result)\n",
    "    print(f\"TEST ACC: {acc_mean:.4f} ± {acc_std:.4f}\", f\", TEST LOSS: {loss_mean:.4f} ± {loss_std:.4f}\")\n",
    "\n",
    "    if validation_ratio > 0.0:\n",
    "        acc_mean, acc_std, loss_mean, loss_std = calc_metric(result, \"valid\")\n",
    "        print(f\"VALID ACC: {acc_mean:.4f} ± {acc_std:.4f}\", f\", VALID LOSS: {loss_mean:.4f} ± {loss_std:.4f}\")\n",
    "\n",
    "    prefix_epsilon_u = list(epsilon_u.items())[:4]\n",
    "    with open(os.path.join(pickle_path, f'online_optimization_{agg_strategy}_{n_users}_{sigma}_{delta}_{dataset_name}_{n_round}_{q_step_size}_{validation_ratio}_{prefix_epsilon_u}_{with_momentum}_{off_train_loss_noise}_{step_decay}_{hp_baseline}.pkl'), 'wb') as file:\n",
    "        pickle.dump(result, file)\n",
    "\n",
    "\n",
    "def show_online_optimization_result(epsilon_u, sigma, delta, n_users, n_round, dataset_name, q_step_size, agg_strategy, validation_ratio=0.0, with_momentum=None, hp_baseline=None, off_train_loss_noise=None, step_decay=True, errorbar=True, img_name=\"\"):\n",
    "    prefix_epsilon_u = list(epsilon_u.items())[:4]\n",
    "    with open(os.path.join(pickle_path, f'online_optimization_{agg_strategy}_{n_users}_{sigma}_{delta}_{dataset_name}_{n_round}_{q_step_size}_{validation_ratio}_{prefix_epsilon_u}_{with_momentum}_{off_train_loss_noise}_{step_decay}_{hp_baseline}.pkl'), 'rb') as file:\n",
    "        result = pickle.load(file)\n",
    "\n",
    "    # eps_uの値のリストを取得（すべての辞書から共通のキーを抽出）\n",
    "    eps_u_values = set(key for dct in result for key in dct[\"param_history\"].keys())\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    for eps_u in eps_u_values:\n",
    "        # 各辞書からeps_uに対応するデータを集める\n",
    "        all_data = np.array([dct[\"param_history\"][eps_u][:-1] for dct in result if eps_u in dct[\"param_history\"]])\n",
    "\n",
    "        # 平均値と標準偏差を計算\n",
    "        means = np.mean(all_data, axis=0)\n",
    "        stds = np.std(all_data, axis=0)\n",
    "\n",
    "        # 平均値と標準偏差をプロット\n",
    "        x = range(len(means))\n",
    "        y1 = [item[0] for item in means]\n",
    "        y2 = [item[1] for item in means]\n",
    "        error1 = [item[0] for item in stds]\n",
    "        error2 = [item[1] for item in stds]\n",
    "\n",
    "        if errorbar:\n",
    "            ax1.errorbar(x, y1, yerr=error1, label=f'eps_u={eps_u} q_u', alpha=0.5)\n",
    "            # ax1.errorbar(x, y2, yerr=error2, label=f'eps_u={eps_u} C_u', alpha=0.5)\n",
    "        else:\n",
    "            ax1.plot(x, y1, label=f'eps_u={eps_u} q_u', alpha=0.5)\n",
    "            # ax1.plot(x, y2, label=f'eps_u={eps_u} C_u', alpha=0.5)\n",
    "\n",
    "    _, ax_train_loss = plt.subplots()\n",
    "    for eps_u in eps_u_values:\n",
    "        # 各辞書からeps_uに対応するデータを集める\n",
    "        all_data = np.array([dct[\"loss_history\"][eps_u][:-1] for dct in result if eps_u in dct[\"loss_history\"]])\n",
    "\n",
    "        # 平均値と標準偏差を計算\n",
    "        means = np.mean(all_data, axis=0)\n",
    "        stds = np.std(all_data, axis=0)\n",
    "\n",
    "        # 平均値と標準偏差をプロット\n",
    "        train_loss_round = range(len(means))\n",
    "        y = [item[0] for item in means]\n",
    "        error = [item[0] for item in stds]\n",
    "\n",
    "        if errorbar:\n",
    "            ax_train_loss.errorbar(train_loss_round, y, yerr=error, label=f'Train Loss eps_u={eps_u} n_users={n_users}', alpha=0.9)\n",
    "        else:\n",
    "            ax_train_loss.plot(train_loss_round, y, label=f'Train Loss eps_u={eps_u} n_users={n_users}', alpha=0.9)\n",
    "\n",
    "        y = [item[1] for item in means]\n",
    "        error = [item[1] for item in stds]\n",
    "\n",
    "        if errorbar:\n",
    "            ax_train_loss.errorbar(train_loss_round, y, yerr=error, label=f'Original Train Loss eps_u={eps_u} n_users={n_users}', alpha=0.9)\n",
    "        else:\n",
    "            ax_train_loss.plot(train_loss_round, y, label=f'Original Train Loss eps_u={eps_u} n_users={n_users}', alpha=0.9)\n",
    "\n",
    "\n",
    "    # ax_train_loss.set_yscale('log')\n",
    "    ax_train_loss.set_xlabel('Round')\n",
    "    ax_train_loss.set_ylabel('Local Train Loss / Train metric')\n",
    "    ax_train_loss.legend(loc='lower left')\n",
    "\n",
    "\n",
    "    loss_means = []\n",
    "    loss_stds = []\n",
    "    acc_means = []\n",
    "    acc_stds = []\n",
    "\n",
    "    # 各ラウンドに対して処理\n",
    "    for i in range(len(result[0]['global']['global_test'])):\n",
    "        # その位置における全ラウンドのloss値を集める\n",
    "        losses_at_position = [result[round_id]['global']['global_test'][i][2] for round_id in range(len(result))]\n",
    "        accs_at_position = [result[round_id]['global']['global_test'][i][1] for round_id in range(len(result))]\n",
    "\n",
    "        loss_means.append(np.mean(losses_at_position))\n",
    "        loss_stds.append(np.std(losses_at_position))\n",
    "\n",
    "        acc_means.append(np.mean(accs_at_position))\n",
    "        acc_stds.append(np.std(accs_at_position))\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    if errorbar:\n",
    "        ax2.errorbar(x, loss_means, yerr=loss_stds, label=f'Test Loss', color='red', alpha=0.5)\n",
    "    else:\n",
    "        ax2.plot(x, loss_means, label=f'Test Loss', color='red', alpha=0.5)\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_xlabel('Round')\n",
    "    ax1.set_ylabel('q_u and C_u')\n",
    "    ax2.set_ylabel('Test Loss')\n",
    "    ax2.set_yscale('log')\n",
    "    plt.title('q_u and C_u with Test Loss')\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    plt.savefig(\n",
    "        os.path.join(img_path, \"online_optimization_result-\" + img_name + \".png\"),\n",
    "        dpi=150,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "\n",
    "    # グラフの表示\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    if errorbar:\n",
    "        ax.errorbar(x, acc_means, yerr=acc_stds, label=f'Test Accuracy', color='red', alpha=0.5)\n",
    "    else:\n",
    "        ax.plot(x, acc_means, label=f'Test Accuracy', color='red', alpha=0.5)\n",
    "    ax.set_xlabel('Round')\n",
    "    ax.set_ylabel('Test Accuracy')\n",
    "    ax.set_title('Test Accuracy')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_specified_idx_result(prefix_epsilon_u_list, sigma, delta, n_users, n_round, dataset_name, q_step_size, idx_per_group_list, label_list=\"\", validation_ratio=0.0, errorbar=True, img_name=\"\"):\n",
    "    # optimal q_u\n",
    "    _, ax_loss = plt.subplots()\n",
    "    if type(idx_per_group_list) is not list:\n",
    "        prefix_epsilon_u_list = [prefix_epsilon_u_list]\n",
    "        idx_per_group_list = [idx_per_group_list]\n",
    "        label_list = [label_list]\n",
    "    for idx_per_group, prefix_epsilon_u, label in zip(idx_per_group_list, prefix_epsilon_u_list, label_list):\n",
    "        with open(os.path.join(pickle_path, f'specified_idx_{n_users}_{sigma}_{delta}_{dataset_name}_{n_round}_{idx_per_group}_{q_step_size}_{validation_ratio}_{prefix_epsilon_u}.pkl'), 'rb') as file:\n",
    "            result = pickle.load(file)\n",
    "\n",
    "        loss_means = []\n",
    "        loss_stds = []\n",
    "        acc_means = []\n",
    "        acc_stds = []\n",
    "\n",
    "        for i in range(len(result[0]['global']['global_test'])):\n",
    "            losses_at_position = [result[round_id]['global']['global_test'][i][2] for round_id in range(len(result))]\n",
    "            accs_at_position = [result[round_id]['global']['global_test'][i][1] for round_id in range(len(result))]\n",
    "\n",
    "            loss_means.append(np.mean(losses_at_position))\n",
    "            loss_stds.append(np.std(losses_at_position))\n",
    "\n",
    "            acc_means.append(np.mean(accs_at_position))\n",
    "            acc_stds.append(np.std(accs_at_position))\n",
    "\n",
    "        x = range(len(loss_means))\n",
    "        if errorbar:\n",
    "            ax_loss.errorbar(x, loss_means, yerr=loss_stds, label=f'{label}', alpha=0.8)\n",
    "        else:\n",
    "            ax_loss.plot(x, loss_means, label=f'{label}', alpha=0.8)\n",
    "\n",
    "    ax_loss.set_ylabel('Test Loss', fontsize=20)\n",
    "    ax_loss.set_xlabel('Round', fontsize=20)\n",
    "    ax_loss.set_yscale('log')\n",
    "    ax_loss.legend(loc='upper right', fontsize=20)\n",
    "\n",
    "    plt.savefig(\n",
    "        os.path.join(img_path, \"specified_idx_result-\" + img_name + \".png\"),\n",
    "        dpi=150,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import noise_utils\n",
    "import time\n",
    "from opacus.accountants.analysis import rdp as analysis\n",
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "ORDERS = RDPAccountant.DEFAULT_ALPHAS\n",
    "\n",
    "def get_privacy_spent(\n",
    "    *, delta: float, history\n",
    "):\n",
    "    alphas = RDPAccountant.DEFAULT_ALPHAS\n",
    "    rdp = sum(\n",
    "        [\n",
    "            analysis.compute_rdp(\n",
    "                q=sample_rate,\n",
    "                noise_multiplier=noise_multiplier,\n",
    "                steps=num_steps,\n",
    "                orders=alphas,\n",
    "            )\n",
    "            for (noise_multiplier, sample_rate, num_steps) in history\n",
    "        ]\n",
    "    )\n",
    "    eps, best_alpha = analysis.get_privacy_spent(\n",
    "        orders=alphas, rdp=rdp, delta=delta\n",
    "    )\n",
    "    return float(eps), float(best_alpha), rdp\n",
    "\n",
    "\n",
    "def get_noise_multiplier_with_history(\n",
    "    sample_rate,\n",
    "    delta,\n",
    "    epsilon_u,\n",
    "    total_round,\n",
    "    current_round,\n",
    "    current_accountant: RDPAccountant,\n",
    "    precision=1e-6,\n",
    "):\n",
    "    max_sigma = 1000\n",
    "    min_sigma = 0\n",
    "    count = 0\n",
    "    start_all_time = time.time()  # 時間測定開始\n",
    "    total_time = 0 \n",
    "    rdp_cache = compute_rdp_from_history(current_accountant.history, cache=True)\n",
    "    while True:\n",
    "        count += 1\n",
    "        sigma = (max_sigma + min_sigma) / 2\n",
    "        noise_multiplier=sigma\n",
    "        print(\"sigma\", sigma)\n",
    "        history = [(noise_multiplier, sample_rate, total_round - current_round)]\n",
    "        start_time = time.time()  # 時間測定開始\n",
    "        updated_rdp = compute_rdp_from_history(history)\n",
    "        eps, _ = analysis.get_privacy_spent(\n",
    "            orders=ORDERS, rdp=rdp_cache + updated_rdp, delta=delta\n",
    "        )\n",
    "        print(\"eps\", eps)\n",
    "        end_time = time.time()  # 時間測定終了\n",
    "        total_time += end_time - start_time  # 合計時間に加算\n",
    "        if eps < epsilon_u:\n",
    "            max_sigma = sigma\n",
    "        else:\n",
    "            min_sigma = sigma\n",
    "        if 0 < epsilon_u - eps and epsilon_u - eps < precision:\n",
    "            end_all_time = time.time()  # 時間測定終了\n",
    "            print(\"current_round\", current_round, \", count\", count, \", get_epsilon() time\", total_time, \", other time\", end_all_time - start_all_time - total_time)\n",
    "            return sigma, eps\n",
    "\n",
    "\n",
    "def from_q_u_with_history(\n",
    "    q_u,\n",
    "    delta,\n",
    "    epsilon_u,\n",
    "    sigma,\n",
    "    total_round,\n",
    "    current_round,\n",
    "    current_accountant: RDPAccountant,\n",
    "    precision=1e-6,\n",
    "):\n",
    "    max_sensitivity_u = 100\n",
    "    min_sensitivity_u = 0\n",
    "    count = 0\n",
    "    start_all_time = time.time()  # 時間測定開始\n",
    "    total_time = 0\n",
    "    rdp_cache = compute_rdp_from_history(current_accountant.history, cache=True)\n",
    "    while True:\n",
    "        count += 1\n",
    "        sensitivity_u = (max_sensitivity_u + min_sensitivity_u) / 2\n",
    "        noise_multiplier=sigma / sensitivity_u\n",
    "        sample_rate=q_u\n",
    "        history = [(noise_multiplier, sample_rate, total_round - current_round)]\n",
    "        start_time = time.time()  # 時間測定開始\n",
    "        updated_rdp = compute_rdp_from_history(history)\n",
    "        eps, _ = analysis.get_privacy_spent(\n",
    "            orders=ORDERS, rdp=rdp_cache + updated_rdp, delta=delta\n",
    "        )\n",
    "        end_time = time.time()  # 時間測定終了\n",
    "        total_time += end_time - start_time  # 合計時間に加算\n",
    "        if eps < epsilon_u:\n",
    "            min_sensitivity_u = sensitivity_u\n",
    "        else:\n",
    "            max_sensitivity_u = sensitivity_u\n",
    "        if 0 < epsilon_u - eps and epsilon_u - eps < precision:\n",
    "            end_all_time = time.time()  # 時間測定終了\n",
    "            print(\"current_round\", current_round, \", count\", count, \", get_epsilon() time\", total_time, \", other time\", end_all_time - start_all_time - total_time)\n",
    "            return sensitivity_u, eps\n",
    "\n",
    "\n",
    "def compute_rdp_from_history(history, cache=False):\n",
    "    if cache:\n",
    "        history_key = tuple(tuple(x) for x in history)\n",
    "\n",
    "        if not hasattr(compute_rdp_from_history, \"cache\"):\n",
    "            compute_rdp_from_history.cache = {}\n",
    "\n",
    "        if history_key in compute_rdp_from_history.cache:\n",
    "            return compute_rdp_from_history.cache[history_key]\n",
    "\n",
    "    rdp = sum(\n",
    "        [\n",
    "            _compute_rdp_with_cache(sample_rate, noise_multiplier, num_steps)\n",
    "            for (noise_multiplier, sample_rate, num_steps) in history\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if cache:\n",
    "        compute_rdp_from_history.cache[history_key] = rdp\n",
    "    return rdp\n",
    "\n",
    "\n",
    "# @lru_cache(maxsize=1024)\n",
    "def _compute_rdp_with_cache(q, noise_multiplier, steps):\n",
    "    rdp = np.array([_compute_single_rdp_with_cache(q, noise_multiplier, order) for order in ORDERS])\n",
    "    return rdp * steps\n",
    "\n",
    "# @lru_cache(maxsize=1024)\n",
    "def _compute_single_rdp_with_cache(q, noise_multiplier, order):\n",
    "    return analysis._compute_rdp(q, noise_multiplier, order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 90\n",
    "sigma = 1.0\n",
    "delta=1e-5\n",
    "eps_u = 1.0\n",
    "\n",
    "q_1 = 1.0\n",
    "q_2 = 0.01\n",
    "# q_2 = 0.9\n",
    "q_3 = 0.001\n",
    "# q_3 = 0.8\n",
    "\n",
    "# # Case.1\n",
    "# print(\"\\n###### Case.1 ######\")\n",
    "# C_1 = from_q_u(q_u=q_1, delta=delta, epsilon_u=eps_u, sigma=sigma, T=T)[0]\n",
    "# print(\"q_1\", q_1, \"C_1\", C_1)\n",
    "# accountant = RDPAccountant()\n",
    "# for i in range(T):\n",
    "#     accountant.step(noise_multiplier=sigma/C_1, sample_rate=q_1)\n",
    "# eps_1, best_alpha_1, rdp_1 = get_privacy_spent(delta=delta, history=accountant.history)\n",
    "# print(\"eps_1\", eps_1, \"best_alpha_1\", best_alpha_1)\n",
    "# print(\"rdp_1\", rdp_1[:15])\n",
    "\n",
    "# accountant = RDPAccountant()\n",
    "# for i in range(1):\n",
    "#     accountant.step(noise_multiplier=sigma/C_1, sample_rate=q_1)\n",
    "# single_eps_1, single_best_alpha_1, single_rdp_1 = get_privacy_spent(delta=delta, history=accountant.history)\n",
    "# print(\"single_eps_1\", single_eps_1, \"single_best_alpha_1\", single_best_alpha_1)\n",
    "# print(\"single_rdp_1\", single_rdp_1[:15])\n",
    "\n",
    "\n",
    "# # Case.2\n",
    "# print(\"\\n###### Case.2 ######\")\n",
    "# C_2 = from_q_u(q_u=q_2, delta=delta, epsilon_u=eps_u, sigma=sigma, T=T)[0]\n",
    "# print(\"q_2\", q_2, \"C_2\", C_2)\n",
    "# accountant = RDPAccountant()\n",
    "# for i in range(T):\n",
    "#     accountant.step(noise_multiplier=sigma/C_2, sample_rate=q_2)\n",
    "# eps_2, best_alpha_2, rdp_2 = get_privacy_spent(delta=delta, history=accountant.history)\n",
    "# print(\"eps_2\", eps_2, \"best_alpha_2\", best_alpha_2)\n",
    "# print(\"rdp_2\", rdp_2[:15])\n",
    "\n",
    "# accountant = RDPAccountant()\n",
    "# for i in range(1):\n",
    "#     accountant.step(noise_multiplier=sigma/C_2, sample_rate=q_2)\n",
    "# single_eps_2, single_best_alpha_2, single_rdp_2 = get_privacy_spent(delta=delta, history=accountant.history)\n",
    "# print(\"single_eps_2\", single_eps_2, \"single_best_alpha_2\", single_best_alpha_2)\n",
    "# print(\"single_rdp_2\", single_rdp_2[:15])\n",
    "\n",
    "# # Case.3\n",
    "# print(\"\\n###### Case.3 ######\")\n",
    "# C_3 = from_q_u(q_u=q_3, delta=delta, epsilon_u=eps_u, sigma=sigma, T=T)[0]\n",
    "# print(\"q_3\", q_3, \"C_3\", C_3)\n",
    "# accountant = RDPAccountant()\n",
    "# for i in range(T):\n",
    "#     accountant.step(noise_multiplier=sigma/C_3, sample_rate=q_3)\n",
    "# eps_3, best_alpha_3, rdp_3 = get_privacy_spent(delta=delta, history=accountant.history)\n",
    "# print(\"eps_3\", eps_3, \"best_alpha_3\", best_alpha_3)\n",
    "# print(\"rdp_3\", rdp_3[:15])\n",
    "\n",
    "# accountant = RDPAccountant()\n",
    "# for i in range(1):\n",
    "#     accountant.step(noise_multiplier=sigma/C_3, sample_rate=q_3)\n",
    "# single_eps_3, single_best_alpha_3, single_rdp_3 = get_privacy_spent(delta=delta, history=accountant.history)\n",
    "# print(\"single_eps_3\", single_eps_3, \"single_best_alpha_3\", single_best_alpha_3)\n",
    "# print(\"single_rdp_3\", single_rdp_3[:15])\n",
    "\n",
    "\n",
    "# Case.4\n",
    "print(\"\\n###### Case.4 ######\")\n",
    "accountant = RDPAccountant()\n",
    "print(\"start 1\")\n",
    "C_1 = from_q_u_with_history(current_accountant=accountant, q_u=q_1, delta=delta, epsilon_u=eps_u, sigma=sigma, total_round=T, current_round=0)[0]\n",
    "print(\"q_1\", q_1, \"C_1\", C_1)\n",
    "for i in range(T//9):\n",
    "    accountant.step(noise_multiplier=sigma/C_1, sample_rate=q_1)\n",
    "    opt_sigma, opt_eps = get_noise_multiplier_with_history(q_1, delta, eps_u, total_round=T, current_round=i*3+1, current_accountant=accountant)\n",
    "    accountant.step(noise_multiplier=opt_sigma, sample_rate=q_1)\n",
    "    opt_sigma, opt_eps = get_noise_multiplier_with_history(q_1, delta, eps_u, total_round=T, current_round=i*3+2, current_accountant=accountant)\n",
    "    accountant.step(noise_multiplier=opt_sigma, sample_rate=q_1)\n",
    "\n",
    "\n",
    "print(\"start 2\")\n",
    "C_2 = from_q_u_with_history(current_accountant=accountant, q_u=q_2, delta=delta, epsilon_u=eps_u, sigma=sigma, total_round=T, current_round=T//3)[0]\n",
    "print(\"q_2\", q_2, \"C_2\", C_2)\n",
    "for i in range(T//9):\n",
    "    accountant.step(noise_multiplier=sigma/C_2, sample_rate=q_2)\n",
    "    opt_sigma, opt_eps = get_noise_multiplier_with_history(q_2, delta, eps_u, total_round=T, current_round=T//3*1+i*3+1, current_accountant=accountant)\n",
    "    accountant.step(noise_multiplier=opt_sigma, sample_rate=q_2)\n",
    "    opt_sigma, opt_eps = get_noise_multiplier_with_history(q_2, delta, eps_u, total_round=T, current_round=T//3*1+i*3+2, current_accountant=accountant)\n",
    "    accountant.step(noise_multiplier=opt_sigma, sample_rate=q_2)\n",
    "\n",
    "\n",
    "print(\"start 3\")\n",
    "C_3 = from_q_u_with_history(current_accountant=accountant, q_u=q_3, delta=delta, epsilon_u=eps_u, sigma=sigma, total_round=T, current_round=T//3 + T//3)[0]\n",
    "print(\"q_3\", q_3, \"C_3\", C_3)\n",
    "for i in range(T//9):\n",
    "    accountant.step(noise_multiplier=sigma/C_3, sample_rate=q_3)\n",
    "    opt_sigma, opt_eps = get_noise_multiplier_with_history(q_3, delta, eps_u, total_round=T, current_round=T//3*2+i*3+1, current_accountant=accountant)\n",
    "    accountant.step(noise_multiplier=opt_sigma, sample_rate=q_3)\n",
    "    opt_sigma, opt_eps = get_noise_multiplier_with_history(q_3, delta, eps_u, total_round=T, current_round=T//3*2+i*3+2, current_accountant=accountant)\n",
    "    accountant.step(noise_multiplier=opt_sigma, sample_rate=q_3)\n",
    "    \n",
    "eps, best_alpha, rdp = get_privacy_spent(delta=delta, history=accountant.history)\n",
    "print(\"eps\", eps, \"best_alpha\", best_alpha)\n",
    "print(\"rdp\", rdp[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check monotonically increasing between q and C for epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 1e-5\n",
    "sigma = 5.0\n",
    "epsilon_u = 5.0\n",
    "T = 1000\n",
    "\n",
    "q_u_list = [0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.8, 0.99]\n",
    "sensitivity_u_list = [0.01, 0.1, 1.0, 5.0, 10, 100]\n",
    "data = []\n",
    "\n",
    "for q_u in q_u_list:\n",
    "    for sensitivity_u in sensitivity_u_list:\n",
    "        func_gaussian = lambda x: RDP_gaussian_with_C(sigma, x, sensitivity_u)\n",
    "        accountant = rdp_acct.anaRDPacct(m=100)\n",
    "        for i in range(T):\n",
    "            accountant.compose_subsampled_mechanisms_lowerbound(func=func_gaussian, prob=q_u)\n",
    "        eps = accountant.get_eps(delta)\n",
    "        if eps < 1e10:\n",
    "            data.append((q_u, sensitivity_u, eps))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "\n",
    "# for q_u in q_u_list\n",
    "fig = plt.figure(figsize=(15, 2))\n",
    "for i, q_u in enumerate(q_u_list):\n",
    "    filtered_data = [d for d in data if d[0] == q_u]\n",
    "    x = [d[1] for d in filtered_data]\n",
    "    y = [d[2] for d in filtered_data]\n",
    "    ax = fig.add_subplot(1, len(q_u_list), i+1)\n",
    "    ax.plot(x, y, label='q_u = {}'.format(q_u), marker='o')\n",
    "    ax.set_title('q_u = {}'.format(q_u), size=14)\n",
    "    ax.set_xlabel('sensitivity_u', size=14)\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('epsilon', size=14)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    # ax.set_ylim(0, epsilon_u+1.0)\n",
    "plt.show()\n",
    "\n",
    "# for sensitivity_u in sensitivity_u_list\n",
    "fig = plt.figure(figsize=(18, 2))\n",
    "for i, sensitivity_u in enumerate(sensitivity_u_list):\n",
    "    filtered_data = [d for d in data if d[1] == sensitivity_u]\n",
    "    x = [d[0] for d in filtered_data]\n",
    "    y = [d[2] for d in filtered_data]\n",
    "    ax = fig.add_subplot(1, len(sensitivity_u_list), i+1)\n",
    "    ax.plot(x, y, label='sensitivity_u = {}'.format(sensitivity_u), marker='o')\n",
    "    ax.set_title('sensitivity_u = {}'.format(sensitivity_u), size=14)\n",
    "    ax.set_xlabel('q_u', size=14)\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('epsilon', size=14)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    # ax.set_ylim(0, epsilon_u+1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Search based method\n",
    "\n",
    "- `accountant = rdp_acct.anaRDPacct(m=m)`\n",
    "    - Zhu, Yuqing, and Yu-Xiang Wang. \"Poission subsampled rényi differential privacy.\" International Conference on Machine Learning. PMLR, 2019.\n",
    "- `accountant = RDPAccountant()`\n",
    "    - Mironov, Ilya, Kunal Talwar, and Li Zhang. \"R\\'enyi differential privacy of the sampled gaussian mechanism.\" arXiv preprint arXiv:1908.10530 (2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 1e-5\n",
    "sigma = 5.0\n",
    "epsilon_u = 5.0\n",
    "T = 100\n",
    "\n",
    "sensitivity_u, eps = from_q_u(q_u=0.1, delta=delta, epsilon_u=epsilon_u, sigma=sigma, T=T)\n",
    "print(\"sensitivity_u =\", sensitivity_u, \"eps =\", eps)\n",
    "\n",
    "q_u, eps = from_sensitivity_u(sensitivity_u=1.59, delta=delta, epsilon_u=epsilon_u, sigma=sigma, T=T)\n",
    "print(\"q_u =\", q_u, \"eps =\", eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation\n",
    "- to observe what happens with various sensitivity and sampling rate\n",
    "\n",
    "Parameters\n",
    "- epsilon_u, delta_u\n",
    "    - privacy budgets for each users\n",
    "- homo, hetero\n",
    "    - distribution of privacy budgets for each users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_u = 3.0\n",
    "n_round = 20\n",
    "delta = 1e-5\n",
    "for sigma in [0.5]:\n",
    "    # x, y = make_q_c_curve(epsilon_u=epsilon_u, delta=delta, sigma=sigma, num_points=30, n_round=n_round, min=-6)\n",
    "    plot_q_c_curve(x, y, title=f\"n_round = {n_round}, epsilon_u = {epsilon_u}, delta = {delta}, sigma = {sigma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_u = 3.0\n",
    "n_round = 10\n",
    "delta = 1e-5\n",
    "for sigma in [0.5]:\n",
    "    x, y = make_q_c_curve(epsilon_u=epsilon_u, delta=delta, sigma=sigma, num_points=30, n_round=n_round, min=-6)\n",
    "    plot_q_c_curve(x, y, title=f\"n_round = {n_round}, epsilon_u = {epsilon_u}, delta = {delta}, sigma = {sigma}\", log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 1e-5\n",
    "# epsilon_u = 1.0\n",
    "# sigma = 0.5\n",
    "# for n_round in [10, 100, 1000]:\n",
    "#     x, y = make_q_c_curve(epsilon_u=epsilon_u, delta=delta, sigma=0.5, num_points=30, n_round=n_round, min=-6)\n",
    "#     plot_q_c_curve(x, y, title=f\"n_round = {n_round}, epsilon_u = {epsilon_u}, delta = {delta}, sigma = {sigma}\")\n",
    "\n",
    "# epsilon_u = 10.0\n",
    "# n_round = 100\n",
    "# for n_round in [10, 100, 1000]:\n",
    "#     x, y = make_q_c_curve(epsilon_u=epsilon_u, delta=delta, sigma=0.5, num_points=30, n_round=n_round, min=-6)\n",
    "#     plot_q_c_curve(x, y, title=f\"n_round = {n_round}, epsilon_u = {epsilon_u}, delta = {delta}, sigma = {sigma}\")\n",
    "\n",
    "\n",
    "epsilon_u = 3.0\n",
    "n_round = 20\n",
    "sigma = 0.5\n",
    "for epsilon_u in [0.5, 1.0, 5.0, 10.0]:\n",
    "    x, y = make_q_c_curve(epsilon_u=epsilon_u, delta=delta, sigma=sigma, num_points=30, n_round=n_round, min=-6)\n",
    "    plot_q_c_curve(x, y, title=f\"n_round = {n_round}, epsilon_u = {epsilon_u}, delta = {delta}, sigma = {sigma}\", img_name=f\"{epsilon_u}\")\n",
    "    plot_q_c_curve(x, y, title=f\"n_round = {n_round}, epsilon_u = {epsilon_u}, delta = {delta}, sigma = {sigma}\", log=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static (Manual) HP search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The relationship between step size and q_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各ステップサイズに対するq_listを計算\n",
    "step_size = [0.8, 0.9]\n",
    "all_q_lists = []\n",
    "for step in step_size:\n",
    "    q_list = []\n",
    "    init_q = 1.0\n",
    "    for _ in range(30):\n",
    "        q_list.append(init_q)\n",
    "        init_q *= step\n",
    "    all_q_lists.append(q_list)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "for i, q_list in enumerate(all_q_lists):\n",
    "    plt.plot(q_list, label=f'Step Size {step_size[i]}', marker='o')\n",
    "plt.yscale('log')\n",
    "plt.title('Q Values Over Time with Different Step Sizes (Log Scale)')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Q Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### heart disease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### various n_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1.0\n",
    "epsilon_list = [0.5]\n",
    "group_thresholds = [0.5]\n",
    "ratio_list = [1.0]\n",
    "delta = 1e-5\n",
    "n_round = 20\n",
    "dataset_name = 'heart_disease'\n",
    "q_step_size = 0.8\n",
    "times = 5\n",
    "validation_ratio = 0.0\n",
    "\n",
    "for n_users in [50, 100, 200, 400]:\n",
    "    random_state = np.random.RandomState(0)\n",
    "    epsilon_u = make_epsilon_u(n_users=n_users, dist=\"hetero\", epsilon_list=epsilon_list, ratio_list=ratio_list, random_state=random_state)\n",
    "    grouped = group_by_closest_below(epsilon_u_dct=epsilon_u, group_thresholds=group_thresholds)\n",
    "    epsilon_u = {}\n",
    "    for eps_u, user_ids in grouped.items():\n",
    "        for user_id in user_ids:\n",
    "            epsilon_u[user_id] = eps_u\n",
    "    opt_strategy = prepare_grid_search(epsilon_u, start_idx=0, end_idx=12)\n",
    "\n",
    "    logger_set_warning()\n",
    "    static_optimization(epsilon_u, sigma, delta, n_users, n_round, dataset_name, times, q_step_size, opt_strategy=opt_strategy, global_learning_rate=10.0, local_learning_rate=0.001, local_epochs=30, validation_ratio=validation_ratio)\n",
    "    min_idx, min_loss = show_static_optimization_result(epsilon_u, sigma, delta, n_users, n_round, dataset_name, q_step_size, n_silos=4, opt_strategy=opt_strategy, validation_ratio=validation_ratio, train_loss=True, img_name=f\"heart_disease-users-{n_users}\")\n",
    "    print(min_idx, min_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### various eps_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1.0\n",
    "ratio_list = [1.0]\n",
    "delta = 1e-5\n",
    "n_round = 20\n",
    "n_users = 400\n",
    "dataset_name = 'heart_disease'\n",
    "q_step_size = 0.8\n",
    "times = 5\n",
    "validation_ratio = 0.0\n",
    "\n",
    "for eps_u in [0.5, 1.0, 10.0]:\n",
    "    epsilon_list = [eps_u]\n",
    "    group_thresholds = [eps_u]\n",
    "    random_state = np.random.RandomState(0)\n",
    "    epsilon_u = make_epsilon_u(n_users=n_users, dist=\"hetero\", epsilon_list=epsilon_list, ratio_list=ratio_list, random_state=random_state)\n",
    "    grouped = group_by_closest_below(epsilon_u_dct=epsilon_u, group_thresholds=group_thresholds)\n",
    "    epsilon_u = {}\n",
    "    for eps_u, user_ids in grouped.items():\n",
    "        for user_id in user_ids:\n",
    "            epsilon_u[user_id] = eps_u\n",
    "    opt_strategy = prepare_grid_search(epsilon_u, start_idx=0, end_idx=12)\n",
    "\n",
    "    logger_set_warning()\n",
    "    static_optimization(epsilon_u, sigma, delta, n_users, n_round, dataset_name, times, q_step_size, opt_strategy=opt_strategy, global_learning_rate=10.0, local_learning_rate=0.001, local_epochs=30, validation_ratio=validation_ratio)\n",
    "    min_idx, min_loss = show_static_optimization_result(epsilon_u, sigma, delta, n_users, n_round, dataset_name, q_step_size, n_silos=4, opt_strategy=opt_strategy, validation_ratio=validation_ratio, train_loss=True, img_name=f\"heart_disease-eps-{epsilon_list[0]}\")\n",
    "    print(min_idx, min_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.5\n",
    "epsilon_list = [3.0]\n",
    "group_thresholds = [3.0]\n",
    "ratio_list = [1.0]\n",
    "delta = 1e-5\n",
    "n_users = 500\n",
    "n_round = 50\n",
    "dataset_name = 'body'\n",
    "q_step_size = 0.6\n",
    "times = 5\n",
    "validation_ratio = 0.8\n",
    "\n",
    "for n_users in [50, 500, 1000]:\n",
    "    random_state = np.random.RandomState(0)\n",
    "    epsilon_u = make_epsilon_u(n_users=n_users, dist=\"hetero\", epsilon_list=epsilon_list, ratio_list=ratio_list, random_state=random_state)\n",
    "    grouped = group_by_closest_below(epsilon_u_dct=epsilon_u, group_thresholds=group_thresholds)\n",
    "    epsilon_u = {}\n",
    "    for eps_u, user_ids in grouped.items():\n",
    "        for user_id in user_ids:\n",
    "            epsilon_u[user_id] = eps_u\n",
    "    opt_strategy = prepare_grid_search(epsilon_u, start_idx=0, end_idx=8)\n",
    "\n",
    "    logger_set_warning()\n",
    "    static_optimization(epsilon_u, sigma, delta, n_users, n_round, dataset_name, times, q_step_size, opt_strategy=opt_strategy, global_learning_rate=5.0, local_learning_rate=0.001, local_epochs=30, validation_ratio=validation_ratio)\n",
    "    min_idx, min_loss = show_static_optimization_result(epsilon_u, sigma, delta, n_users, n_round, dataset_name, q_step_size, n_silos=5, opt_strategy=opt_strategy, validation_ratio=validation_ratio, train_loss=True, img_name=f\"{dataset_name}-users-{n_users}\")\n",
    "    print(min_idx, min_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.5\n",
    "ratio_list = [1.0]\n",
    "delta = 1e-5\n",
    "n_users = 500\n",
    "n_round = 50\n",
    "dataset_name = 'body'\n",
    "q_step_size = 0.6\n",
    "times = 5\n",
    "validation_ratio = 0.8\n",
    "\n",
    "for eps_u in [1.0, 3.0, 10.0]:\n",
    "    epsilon_list = [eps_u]\n",
    "    group_thresholds = [eps_u]\n",
    "    random_state = np.random.RandomState(0)\n",
    "    epsilon_u = make_epsilon_u(n_users=n_users, dist=\"hetero\", epsilon_list=epsilon_list, ratio_list=ratio_list, random_state=random_state)\n",
    "    grouped = group_by_closest_below(epsilon_u_dct=epsilon_u, group_thresholds=group_thresholds)\n",
    "    epsilon_u = {}\n",
    "    for eps_u, user_ids in grouped.items():\n",
    "        for user_id in user_ids:\n",
    "            epsilon_u[user_id] = eps_u\n",
    "    opt_strategy = prepare_grid_search(epsilon_u, start_idx=0, end_idx=8)\n",
    "\n",
    "    logger_set_warning()\n",
    "    static_optimization(epsilon_u, sigma, delta, n_users, n_round, dataset_name, times, q_step_size, opt_strategy=opt_strategy, global_learning_rate=5.0, local_learning_rate=0.001, local_epochs=30, validation_ratio=validation_ratio)\n",
    "    min_idx, min_loss = show_static_optimization_result(epsilon_u, sigma, delta, n_users, n_round, dataset_name, q_step_size, n_silos=5, opt_strategy=opt_strategy, validation_ratio=validation_ratio, train_loss=True, img_name=f\"body-eps-{epsilon_list[0]}\")\n",
    "    print(min_idx, min_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creditcard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1.0\n",
    "epsilon_list = [3.0]\n",
    "group_thresholds = [3.0]\n",
    "ratio_list = [1.0]\n",
    "delta = 1e-5\n",
    "n_users = 2000\n",
    "n_round = 20\n",
    "dataset_name = 'creditcard'\n",
    "q_step_size = 0.8\n",
    "times = 3\n",
    "validation_ratio = 0.8\n",
    "\n",
    "random_state = np.random.RandomState(0)\n",
    "epsilon_u = make_epsilon_u(n_users=n_users, dist=\"hetero\", epsilon_list=epsilon_list, ratio_list=ratio_list, random_state=random_state)\n",
    "grouped = group_by_closest_below(epsilon_u_dct=epsilon_u, group_thresholds=group_thresholds)\n",
    "epsilon_u = {}\n",
    "for eps_u, user_ids in grouped.items():\n",
    "    for user_id in user_ids:\n",
    "        epsilon_u[user_id] = eps_u\n",
    "opt_strategy = prepare_grid_search(epsilon_u, start_idx=0, end_idx=8)\n",
    "\n",
    "logger_set_warning()\n",
    "static_optimization(epsilon_u, sigma, delta, n_users, n_round, dataset_name, times, q_step_size, opt_strategy=opt_strategy, global_learning_rate=20.0, local_learning_rate=0.001, local_epochs=20, validation_ratio=validation_ratio)\n",
    "min_idx, min_loss = show_static_optimization_result(epsilon_u, sigma, delta, n_users, n_round, dataset_name, q_step_size, opt_strategy=opt_strategy, validation_ratio=validation_ratio, train_loss=True)\n",
    "print(min_idx, min_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online HP search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with DP train metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### heart disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = 400\n",
    "sigma = 1.0\n",
    "epsilon_list = [0.15, 3.0, 5.0]\n",
    "group_thresholds = epsilon_list\n",
    "ratio_list = [0.34, 0.43, 0.23]\n",
    "delta = 1e-5\n",
    "n_round = 20\n",
    "dataset_name = 'heart_disease'\n",
    "q_step_size = 0.8\n",
    "validation_ratio = 0.0\n",
    "times = 5\n",
    "random_state = np.random.RandomState(0)\n",
    "agg_strategy = \"PULDP-AVG-QCTrain\"\n",
    "with_momentum = True\n",
    "step_decay = True\n",
    "hp_baseline = None\n",
    "\n",
    "logger_set_info()\n",
    "\n",
    "epsilon_u_dct = make_epsilon_u(n_users=n_users, dist=\"hetero\", epsilon_list=epsilon_list, ratio_list=ratio_list, random_state=random_state)\n",
    "grouped = group_by_closest_below(epsilon_u_dct=epsilon_u_dct, group_thresholds=group_thresholds)\n",
    "epsilon_u = {}\n",
    "for eps_u, user_ids in grouped.items():\n",
    "    for user_id in user_ids:\n",
    "        epsilon_u[user_id] = eps_u\n",
    "run_online_optimization(\n",
    "    epsilon_u, sigma, delta, n_users, n_round, dataset_name, q_step_size, \n",
    "    group_thresholds, times, global_learning_rate=10.0, local_learning_rate=0.001, \n",
    "    local_epochs=30, validation_ratio=validation_ratio, agg_strategy=agg_strategy, with_momentum=with_momentum, step_decay=step_decay, hp_baseline=hp_baseline)\n",
    "show_online_optimization_result(\n",
    "    epsilon_u, sigma, delta, n_users, n_round, dataset_name, q_step_size, \n",
    "    validation_ratio=validation_ratio, agg_strategy=agg_strategy, with_momentum=with_momentum, \n",
    "    step_decay=step_decay, hp_baseline=hp_baseline, errorbar=False, img_name=f\"{dataset_name}-users-{n_users}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = 1000\n",
    "sigma = 0.5\n",
    "epsilon_list = [0.15, 3.0, 5.0]\n",
    "group_thresholds = epsilon_list\n",
    "ratio_list = [0.34, 0.43, 0.23]\n",
    "delta = 1e-5\n",
    "n_round = 30\n",
    "dataset_name = 'body'\n",
    "q_step_size = 0.8\n",
    "validation_ratio = 0.0\n",
    "times = 5\n",
    "random_state = np.random.RandomState(0)\n",
    "agg_strategy = \"PULDP-AVG-QCTrain\"\n",
    "with_momentum = True\n",
    "step_decay = True\n",
    "hp_baseline = None\n",
    "\n",
    "logger_set_warning()\n",
    "\n",
    "epsilon_u = make_epsilon_u(n_users=n_users, dist=\"hetero\", epsilon_list=epsilon_list, ratio_list=ratio_list, random_state=random_state)\n",
    "grouped = group_by_closest_below(epsilon_u_dct=epsilon_u, group_thresholds=group_thresholds)\n",
    "epsilon_u = {}\n",
    "for eps_u, user_ids in grouped.items():\n",
    "    for user_id in user_ids:\n",
    "        epsilon_u[user_id] = eps_u\n",
    "run_online_optimization(\n",
    "    epsilon_u, sigma, delta, n_users, n_round, dataset_name, q_step_size, \n",
    "    group_thresholds, times, global_learning_rate=10.0, local_learning_rate=0.01, local_epochs=30, \n",
    "    validation_ratio=validation_ratio, agg_strategy=agg_strategy, with_momentum=with_momentum, step_decay=step_decay, hp_baseline=hp_baseline)\n",
    "show_online_optimization_result(\n",
    "    epsilon_u, sigma, delta, n_users, n_round, dataset_name, q_step_size, \n",
    "    validation_ratio=validation_ratio, agg_strategy=agg_strategy, with_momentum=with_momentum, \n",
    "    step_decay=step_decay, hp_baseline=hp_baseline, errorbar=False, img_name=f\"{dataset_name}-users-{n_users}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = 1000\n",
    "sigma = 0.5\n",
    "epsilon_list = [0.15, 3.0, 5.0]\n",
    "group_thresholds = epsilon_list\n",
    "ratio_list = [0.34, 0.43, 0.23]\n",
    "delta = 1e-5\n",
    "n_round = 30\n",
    "dataset_name = 'body'\n",
    "q_step_size = 0.8\n",
    "validation_ratio = 0.0\n",
    "times = 5\n",
    "random_state = np.random.RandomState(0)\n",
    "agg_strategy = \"PULDP-AVG-QCTest\"\n",
    "with_momentum = True\n",
    "step_decay = True\n",
    "hp_baseline = None\n",
    "\n",
    "logger_set_warning()\n",
    "\n",
    "epsilon_u = make_epsilon_u(n_users=n_users, dist=\"hetero\", epsilon_list=epsilon_list, ratio_list=ratio_list, random_state=random_state)\n",
    "grouped = group_by_closest_below(epsilon_u_dct=epsilon_u, group_thresholds=group_thresholds)\n",
    "epsilon_u = {}\n",
    "for eps_u, user_ids in grouped.items():\n",
    "    for user_id in user_ids:\n",
    "        epsilon_u[user_id] = eps_u\n",
    "run_online_optimization(\n",
    "    epsilon_u, sigma, delta, n_users, n_round, dataset_name, q_step_size, \n",
    "    group_thresholds, times, global_learning_rate=10.0, local_learning_rate=0.01, local_epochs=30, \n",
    "    validation_ratio=validation_ratio, agg_strategy=agg_strategy, with_momentum=with_momentum, step_decay=step_decay, hp_baseline=hp_baseline)\n",
    "show_online_optimization_result(\n",
    "    epsilon_u, sigma, delta, n_users, n_round, dataset_name, q_step_size, \n",
    "    validation_ratio=validation_ratio, agg_strategy=agg_strategy, with_momentum=with_momentum, \n",
    "    step_decay=step_decay, hp_baseline=hp_baseline, errorbar=False, img_name=f\"{dataset_name}-users-{n_users}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = 1000\n",
    "sigma = 0.5\n",
    "epsilon_list = [0.15, 3.0, 5.0]\n",
    "group_thresholds = epsilon_list\n",
    "ratio_list = [0.34, 0.43, 0.23]\n",
    "delta = 1e-5\n",
    "n_round = 30\n",
    "dataset_name = 'body'\n",
    "q_step_size = 0.8\n",
    "validation_ratio = 0.0\n",
    "times = 5\n",
    "random_state = np.random.RandomState(0)\n",
    "agg_strategy = \"PULDP-AVG-QCTest\"\n",
    "with_momentum = True\n",
    "step_decay = True\n",
    "hp_baseline = None\n",
    "off_train_loss_noise = True\n",
    "\n",
    "logger_set_warning()\n",
    "\n",
    "epsilon_u = make_epsilon_u(n_users=n_users, dist=\"hetero\", epsilon_list=epsilon_list, ratio_list=ratio_list, random_state=random_state)\n",
    "grouped = group_by_closest_below(epsilon_u_dct=epsilon_u, group_thresholds=group_thresholds)\n",
    "epsilon_u = {}\n",
    "for eps_u, user_ids in grouped.items():\n",
    "    for user_id in user_ids:\n",
    "        epsilon_u[user_id] = eps_u\n",
    "run_online_optimization(\n",
    "    epsilon_u, sigma, delta, n_users, n_round, dataset_name, q_step_size, \n",
    "    group_thresholds, times, global_learning_rate=10.0, local_learning_rate=0.01, local_epochs=30, off_train_loss_noise=off_train_loss_noise,\n",
    "    validation_ratio=validation_ratio, agg_strategy=agg_strategy, with_momentum=with_momentum, step_decay=step_decay, hp_baseline=hp_baseline)\n",
    "show_online_optimization_result(\n",
    "    epsilon_u, sigma, delta, n_users, n_round, dataset_name, q_step_size, \n",
    "    validation_ratio=validation_ratio, agg_strategy=agg_strategy, with_momentum=with_momentum, off_train_loss_noise=off_train_loss_noise,\n",
    "    step_decay=step_decay, hp_baseline=hp_baseline, errorbar=False, img_name=f\"{dataset_name}-users-{n_users}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXP 6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger_set_warning()\n",
    "\n",
    "conservative_eps = 0.15\n",
    "normal_eps = 1.0\n",
    "liberal_eps = 5.0\n",
    "\n",
    "n_users = 400\n",
    "sigma = 1.0\n",
    "epsilon_list = [conservative_eps, normal_eps, liberal_eps]\n",
    "group_thresholds = epsilon_list\n",
    "ratio_list = [0.34, 0.43, 0.23]\n",
    "delta = 1e-5\n",
    "n_round = 20\n",
    "dataset_name = 'heart_disease'\n",
    "q_step_size = 0.8\n",
    "validation_ratio = 0.0\n",
    "times = 5\n",
    "random_state = np.random.RandomState(0)\n",
    "agg_strategy = \"PULDP-AVG\"\n",
    "with_momentum = False\n",
    "train_loss_dp = False\n",
    "sigma_for_online_optimization = None\n",
    "total_dp_eps_for_online_optimization = False\n",
    "hp_baseline = None\n",
    "\n",
    "prefix_epsilon_u_list = []\n",
    "idx_per_group_list = []\n",
    "\n",
    "for i in [1]:\n",
    "    for j in [1]:\n",
    "        for k in [1]:\n",
    "            idx_per_group = {conservative_eps: i, normal_eps: j, liberal_eps: k}\n",
    "            idx_per_group_list.append(idx_per_group)\n",
    "            print(idx_per_group)\n",
    "            epsilon_u = make_epsilon_u(n_users=n_users, dist=\"hetero\", epsilon_list=epsilon_list, ratio_list=ratio_list, random_state=random_state)\n",
    "            grouped = group_by_closest_below(epsilon_u_dct=epsilon_u, group_thresholds=group_thresholds)\n",
    "            epsilon_u = {}\n",
    "            for eps_u, user_ids in grouped.items():\n",
    "                for user_id in user_ids:\n",
    "                    epsilon_u[user_id] = eps_u\n",
    "            prefix_epsilon_u = list(epsilon_u.items())[:4]\n",
    "            prefix_epsilon_u_list.append(prefix_epsilon_u)\n",
    "            # run_with_specified_idx(epsilon_u, sigma, delta, n_users, n_round, dataset_name, q_step_size, times, \n",
    "            #     idx_per_group=idx_per_group,  global_learning_rate=10.0, local_learning_rate=0.001, local_epochs=50)\n",
    "            show_specified_idx_result(prefix_epsilon_u, sigma, delta, n_users, n_round, dataset_name, q_step_size, idx_per_group, validation_ratio=validation_ratio, img_name=f\"{dataset_name}-puldlpavg\", errorbar=False)\n",
    "\n",
    "\n",
    "epsilon_list = [conservative_eps]\n",
    "group_thresholds = epsilon_list\n",
    "ratio_list = [1.0]\n",
    "\n",
    "for i in [9]:\n",
    "    idx_per_group = {conservative_eps: i}\n",
    "    idx_per_group_list.append(idx_per_group)\n",
    "    print(idx_per_group)\n",
    "    epsilon_u = make_epsilon_u(n_users=n_users, dist=\"hetero\", epsilon_list=epsilon_list, ratio_list=ratio_list, random_state=random_state)\n",
    "    grouped = group_by_closest_below(epsilon_u_dct=epsilon_u, group_thresholds=group_thresholds)\n",
    "    epsilon_u = {}\n",
    "    for eps_u, user_ids in grouped.items():\n",
    "        for user_id in user_ids:\n",
    "            epsilon_u[user_id] = eps_u\n",
    "    prefix_epsilon_u = list(epsilon_u.items())[:4]\n",
    "    prefix_epsilon_u_list.append(prefix_epsilon_u)\n",
    "    # run_with_specified_idx(epsilon_u, sigma, delta, n_users, n_round, dataset_name, q_step_size, times, \n",
    "    #     idx_per_group=idx_per_group,  global_learning_rate=10.0, local_learning_rate=0.001, local_epochs=50)\n",
    "    show_specified_idx_result(prefix_epsilon_u, sigma, delta, n_users, n_round, dataset_name, q_step_size, idx_per_group, validation_ratio=validation_ratio, img_name=f\"{dataset_name}-uldlpavg\", errorbar=False)\n",
    "\n",
    "show_specified_idx_result(prefix_epsilon_u_list, sigma, delta, n_users, n_round, dataset_name, q_step_size, idx_per_group_list, label_list=[\"PULDP-AVG\", \"ULDP-AVG\"], validation_ratio=validation_ratio, img_name=f\"{dataset_name}-comparison\", errorbar=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXP 6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heart disease with different delta\n",
    "n_users_list = [400]\n",
    "sigma = 1.0\n",
    "epsilon_list = [1.0, 3.0, 5.0]\n",
    "group_thresholds = epsilon_list\n",
    "ratio_list = [0.34, 0.43, 0.23]\n",
    "delta = 1e-5\n",
    "n_round = 20\n",
    "dataset_name = 'heart_disease'\n",
    "q_step_size = 0.8\n",
    "validation_ratio = 0.0\n",
    "times = 5\n",
    "random_state = np.random.RandomState(0)\n",
    "agg_strategy = \"PULDP-AVG-online-TRAIN\"\n",
    "with_momentum = True\n",
    "train_loss_dp = True\n",
    "sigma_for_online_optimization = None\n",
    "total_dp_eps_for_online_optimization = True\n",
    "hp_baseline = None\n",
    "\n",
    "\n",
    "prefix_epsilon_u_list = []\n",
    "for n_users in n_users_list:\n",
    "    print(\"n_users:\", n_users)\n",
    "    epsilon_u = make_epsilon_u(n_users=n_users, dist=\"hetero\", epsilon_list=epsilon_list, ratio_list=ratio_list, random_state=random_state)\n",
    "    prefix_epsilon_u_list.append(list(epsilon_u.items())[:4])\n",
    "    grouped = group_by_closest_below(epsilon_u_dct=epsilon_u, group_thresholds=group_thresholds)\n",
    "    epsilon_u = {}\n",
    "    for eps_u, user_ids in grouped.items():\n",
    "        for user_id in user_ids:\n",
    "            epsilon_u[user_id] = eps_u\n",
    "    run_online_optimization(\n",
    "        epsilon_u, sigma, delta, n_users, n_round, dataset_name, q_step_size, \n",
    "        group_thresholds, times, global_learning_rate=10.0, local_learning_rate=0.001, \n",
    "        local_epochs=50, validation_ratio=validation_ratio, agg_strategy=agg_strategy, with_momentum=with_momentum, \n",
    "        train_loss_dp=train_loss_dp, sigma_for_online_optimization=sigma_for_online_optimization, \n",
    "        total_dp_eps_for_online_optimization=total_dp_eps_for_online_optimization, hp_baseline=hp_baseline)\n",
    "    show_online_optimization_result(\n",
    "        epsilon_u, sigma, delta, n_users, n_round, dataset_name, q_step_size, \n",
    "        validation_ratio=validation_ratio, agg_strategy=agg_strategy, with_momentum=with_momentum, \n",
    "        train_loss_dp=train_loss_dp, sigma_for_online_optimization=sigma_for_online_optimization, \n",
    "        total_dp_eps_for_online_optimization=total_dp_eps_for_online_optimization, hp_baseline=hp_baseline, errorbar=False, img_name=f\"{dataset_name}-users-{n_users}\")\n",
    "\n",
    "# show_online_optimization_result_from_n_users_list(prefix_epsilon_u_list, n_users_list, sigma, delta, n_round, dataset_name, q_step_size, validation_ratio=0.9, agg_strategy=agg_strategy, with_momentum=with_momentum, train_loss_dp=train_loss_dp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acsilo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
